{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcb94047-58a4-4fb0-be2c-933fc76d214d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasets==2.20.0 transformers==5.0.0 tf-keras==2.17.0 accelerate==1.4.0 mlflow==2.20.2 torchvision==0.20.1 deepspeed==0.14.4\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bff76e4-a9df-4863-8f27-84fa94e81669",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"dbfs:/Volumes/smart_claims_dev/00_landing/training_imgs/\"\n",
    "training_df = spark.readStream.table(\"smart_claims_dev.02_silver.training_images\")\n",
    "# display(training_df.limit(1), checkpointLocation = checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d422828-d063-4e4c-a2f0-6eeff8ceafb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from pyspark.sql.functions import pandas_udf, col\n",
    "IMAGE_RESIZE = 224\n",
    "\n",
    "landing_catalog = \"smart_claims_dev\"\n",
    "landing_schema = \"00_landing\"\n",
    "base_path = f\"/Volumes/{landing_catalog}/{landing_schema}/claims\"\n",
    "metadata_path = f\"{base_path}/autoloader_metadata\"\n",
    "\n",
    "#Resize UDF function\n",
    "@pandas_udf(\"binary\")\n",
    "def resize_image_udf(content_series):\n",
    "  def resize_image(content):\n",
    "    from PIL import Image\n",
    "    \"\"\"resize image and serialize back as jpeg\"\"\"\n",
    "    #Load the PIL image\n",
    "    image = Image.open(io.BytesIO(content))\n",
    "    width, height = image.size   # Get dimensions\n",
    "    new_size = min(width, height)\n",
    "    # Crop the center of the image\n",
    "    image = image.crop(((width - new_size)/2, (height - new_size)/2, (width + new_size)/2, (height + new_size)/2))\n",
    "    #Resize to the new resolution\n",
    "    image = image.resize((IMAGE_RESIZE, IMAGE_RESIZE), Image.NEAREST)\n",
    "    #Save back as jpeg\n",
    "    output = io.BytesIO()\n",
    "    image.save(output, format='JPEG')\n",
    "    return output.getvalue()\n",
    "  return content_series.apply(resize_image)\n",
    "\n",
    "\n",
    "# add the metadata to enable the image preview\n",
    "image_meta = {\"spark.contentAnnotation\" : '{\"mimeType\": \"image/jpeg\"}'}\n",
    "\n",
    "(training_df\n",
    "      .withColumn(\"content\", resize_image_udf(col(\"content\")).alias(\"content\", metadata=image_meta))\n",
    "      .writeStream\n",
    "      .option(\"checkpointLocation\", f\"{metadata_path}/_checkpoint2\")\n",
    "      .trigger(availableNow=True)\n",
    "      .toTable(\"smart_claims_dev.02_silver.training_images_resized\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ba330a-a403-4899-b72f-bc4baf67c42d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"smart_claims_dev.02_silver.training_images_resized\").limit(10), checkpointLocation = checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1a5759-8154-4e20-b3c7-be9f0b38a5e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import mlflow\n",
    "\n",
    "#Setup the training experiment\n",
    "mlflow.set_experiment(\"/Users/kryshtopenko@gmail.com/image-claims-classifier\")\n",
    "\n",
    "# Convert Spark DataFrame to pandas first (Serverless doesn't support from_spark)\n",
    "pandas_df = spark.table(\"smart_claims_dev.02_silver.training_images_resized\").toPandas()\n",
    "dataset = Dataset.from_pandas(pandas_df).rename_column(\"content\", \"image\")\n",
    "\n",
    "splits = dataset.train_test_split(test_size=0.2, seed = 42)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8422f904-2e07-49b6-bd00-924cd82db61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoImageProcessor\n",
    "\n",
    "# pre-trained model from which to fine-tune\n",
    "# Check the hugging face repo for more details & models: https://huggingface.co/microsoft/resnet-50\n",
    "model_checkpoint = \"microsoft/resnet-50\"\n",
    "\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision.transforms import CenterCrop, Compose, Normalize, RandomResizedCrop, Resize, ToTensor, Lambda\n",
    "\n",
    "#Extract the model feature (contains info on pre-process step required to transform our data, such as resizing & normalization)\n",
    "#Using the model parameters makes it easy to switch to another model without any change, even if the input size is different.\n",
    "model_def = AutoFeatureExtractor.from_pretrained(model_checkpoint)\n",
    "\n",
    "#Transformations on our training dataset. we'll add some crop here\n",
    "transforms = Compose([Lambda(lambda b: Image.open(io.BytesIO(b)).convert(\"RGB\")), #byte to pil\n",
    "                        ToTensor(), #convert the PIL img to a tensor\n",
    "                        Normalize(mean=model_def.image_mean, std=model_def.image_std)\n",
    "                        ])\n",
    "\n",
    "# Add some random resiz & transformation to our training dataset\n",
    "def preprocess(batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    batch[\"image\"] = [transforms(image) for image in batch[\"image\"]]\n",
    "    return batch\n",
    "   \n",
    "#Set our training / validation transformations\n",
    "train_ds.set_transform(preprocess)\n",
    "val_ds.set_transform(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff7039c-4d53-43b8-83c9-5fe90d9eaaa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "#Mapping between class label and value (huggingface use it during inference to output the proper label)\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(set(dataset['label'])):\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label\n",
    "    \n",
    "#Load the base model from its checkpoint\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint, \n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    num_labels=len(label2id),\n",
    "    ignore_mismatched_sizes = True # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca44349-c2ea-4846-8214-015562ace456",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "# 1. Kill the 'phone home' features that cause hangs\n",
    "os.environ[\"DATABRICKS_AUTOLOGGING_ENABLED\"] = \"false\"\n",
    "os.environ[\"REPORT_TO\"] = \"none\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# model_name = model_checkpoint.split(\"/\")[-1]\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    #  f\"/tmp/huggingface/pcb/{model_name}-finetuned\",\n",
    "    # no_cuda=True, #Run on CPU for resnet to make it easier\n",
    "    output_dir=\"/tmp/checkpoints\",\n",
    "    push_to_hub= False,\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=32, \n",
    "    per_device_eval_batch_size=32,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    num_train_epochs=1,  #20\n",
    "    max_steps=10,\n",
    "    load_best_model_at_end=False,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=1,\n",
    "    logging_first_step=True\n",
    ")\n",
    "print(\"Success! Arguments are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae07952a-6cff-4d45-a17b-5bc5a06aba62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# args = TrainingArguments(\n",
    "#     # f\"/tmp/huggingface/pcb/{model_name}-finetuned\",\n",
    "#     # no_cuda=True, #Run on CPU for resnet to make it easier\n",
    "#     remove_unused_columns=False,\n",
    "#     per_device_train_batch_size=32, \n",
    "#     per_device_eval_batch_size=32,\n",
    "#     evaluation_strategy = \"epoch\",\n",
    "#     save_strategy = \"epoch\",\n",
    "#     num_train_epochs=1,  #20\n",
    "#     max_steps=10,\n",
    "#     load_best_model_at_end=True,\n",
    "#     learning_rate=5e-5,\n",
    "#     logging_steps=1,\n",
    "#     logging_first_step=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f31b8ef-b441-429f-92c0-bae719d400f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# This wrapper adds steps before and after the inference to simplify the model usage\n",
    "# Before calling the model: apply the same transform as the training, resizing the image\n",
    "# After callint the model: only keeps the main class with the probability as output\n",
    "class ModelWrapper(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, pipeline):\n",
    "        self.pipeline = pipeline\n",
    "        # instantiate model in evaluation mode\n",
    "        self.pipeline.model.eval()\n",
    "\n",
    "    def predict(self, context, images):\n",
    "        from PIL import Image\n",
    "        with torch.set_grad_enabled(False):\n",
    "            #Convert the byte to PIL images\n",
    "            images = images['content'].apply(lambda b: Image.open(io.BytesIO(b))).to_list()\n",
    "            #the pipeline returns the probability for all the class\n",
    "            predictions = self.pipeline.predict(images)\n",
    "            #Filter & returns only the class with the highest score [{'score': 0.999038815498352, 'label': 'normal'}, ...]\n",
    "            return pd.DataFrame([max(r, key=lambda x: x['score']) for r in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c32f830-03b1-42e9-83c4-0f5357dc7215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, DefaultDataCollator, EarlyStoppingCallback\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "with mlflow.start_run(run_name=\"hugging_face_new\") as run:\n",
    "    mlflow.log_input(mlflow.data.from_huggingface(train_ds, \"training\"))\n",
    "\n",
    "    # use real class count instead of 3\n",
    "    def collate_fn(examples):\n",
    "        import torch\n",
    "        pixel_values = torch.stack([e[\"image\"] for e in examples])\n",
    "        labels = torch.tensor([label2id[e[\"label\"]] for e in examples], dtype=torch.long)\n",
    "        labels = torch.nn.functional.one_hot(labels, num_classes=len(label2id)).float()\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "    trainer = Trainer(model, args, train_dataset=train_ds, eval_dataset=val_ds, tokenizer=model_def, data_collator=collate_fn)\n",
    "    train_results = trainer.train()\n",
    "\n",
    "    # Build final HF pipeline\n",
    "    classifier = pipeline(\"image-classification\", model=trainer.state.best_model_checkpoint, tokenizer=model_def)\n",
    "\n",
    "    # ---- moved from your Cell B, so it's inside the SAME run ----\n",
    "    import pandas as pd\n",
    "    wrapped_model = ModelWrapper(classifier)\n",
    "    test_df = spark.table(\"smart_claims_dev.02_silver.training_images_resized\").select('content').toPandas()\n",
    "    predictions = wrapped_model.predict(None, test_df)\n",
    "    signature = infer_signature(test_df, predictions)\n",
    "\n",
    "    reqs = mlflow.transformers.get_default_pip_requirements(model)\n",
    "\n",
    "    # LOG the model and CAPTURE the URI\n",
    "    logged = mlflow.pyfunc.log_model(\n",
    "        artifact_path=\"model\",\n",
    "        python_model=wrapped_model,\n",
    "        pip_requirements=reqs,\n",
    "        signature=signature,\n",
    "    )\n",
    "\n",
    "# keep these prints to sanity-check\n",
    "from mlflow import artifacts\n",
    "print(\"logged.model_uri:\", logged.model_uri)   # e.g., runs:/<run_id>/model\n",
    "print(\"logged.run_id  :\", logged.run_id)\n",
    "print(\"model files    :\", artifacts.list_artifacts(logged.model_uri))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46d70b33-fd8e-438c-9148-34f9d9460b6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.tracking import MlflowClient\n",
    "import mlflow\n",
    "\n",
    "mlflow.set_registry_uri(\"databricks-uc\")\n",
    "model_name = \"smart_claims_dev.03_gold.claims_damage_level\"\n",
    "\n",
    "registered = mlflow.register_model(\n",
    "    model_uri=logged.model_uri,\n",
    "    name=model_name,\n",
    ")\n",
    "\n",
    "MlflowClient().set_registered_model_alias(\n",
    "    name=model_name,\n",
    "    alias=\"prod\",\n",
    "    version=registered.version,\n",
    ")\n",
    "\n",
    "print(f\"Registered {model_name} v{registered.version} and set alias 'prod'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e3347f-d1f2-4754-8e08-14c3e376c6ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_damage_udf = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/smart_claims_dev.03_gold.claims_damage_level@prod\")\n",
    "columns = predict_damage_udf.metadata.get_input_schema().input_names()\n",
    "#Run the inferences\n",
    "spark.table('smart_claims_dev.02_silver.training_images_resized').withColumn(\"damage_prediction\", predict_damage_udf(*columns)).write.mode('overwrite').saveAsTable('smart_claims_dev.03_gold.damage_predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e0d893a-1eec-4a6f-9d3b-f6666616a800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predictions = spark.table('smart_claims_dev.03_gold.damage_predictions')\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8241f69c-fa8f-488e-8ab7-39aa67cb9ba9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results = predictions.selectExpr(\"path\", \"label\", \"damage_prediction.label as predictions\", \"damage_prediction.score as score\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89fdf505-f9b1-45e5-8845-6213984289b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# create confusion matrix\n",
    "confusion_matrix = pd.crosstab(results['label'], results['predictions'])\n",
    "\n",
    "# plot confusion matrix\n",
    "fig = plt.figure()\n",
    "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\", fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "182ac94d-8ae5-4454-b2da-61d24ae6b539",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_images = (spark.read.table(\"smart_claims_dev.02_silver.claim_images\")\n",
    "                   .withColumn(\"damage_prediction\", predict_damage_udf(*columns)))\n",
    "\n",
    "metadata = spark.table(\"smart_claims_dev.01_bronze.claim_images_meta\")\n",
    "\n",
    "raw_images.join(metadata, on=\"image_name\").write.mode('overwrite').saveAsTable(\"smart_claims_dev.03_gold.claim_images_predicted\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "STANDARD"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "transformers==5.0.0"
    ],
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ML_Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
